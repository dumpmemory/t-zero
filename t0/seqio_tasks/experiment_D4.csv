HF_name,subset,task_by_convention,format,comment,seed_paper,do_train,do_eval,train_size,adjusted_train_size,metric,multiple correct answer,Paper link,non_linguistic_knowledge,Imported Task Name,imported category,input_length,_human_skill,Domain,Reference,done
crows_pairs,,bias_and_fairness,,test set only; authors themselves acknowledge some problems,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
jigsaw_toxicity_pred,,bias_and_fairness,,current https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data ; want https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
super_glue,axg,bias_and_fairness,cls,test set only,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
wino_bias,type1_anti,bias_and_fairness,cls,,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
wino_bias,type2_anti,bias_and_fairness,cls,,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
wino_bias,type1_pro,bias_and_fairness,cls,,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
wino_bias,type2_pro,bias_and_fairness,cls,,Eval WG,,BIAS_FAIRNESS,,,,,,,,,,,,,TRUE
super_glue,wsc.fixed,coreference,cls,,,SGLUE,BASE,554,0,accuracy,,https://arxiv.org/pdf/1905.00537.pdf,,superglue-wsc,cls/other,single sentence,knowledge-? reading comprehension,,Levesque et al. 2012,TRUE
winogrande,winogrande_xl,coreference,ext,,GPT,,BASE,40398,0,accuracy,,https://arxiv.org/pdf/1907.10641.pdf,,WinoGrande,qa/multiple-choice qa,,knowledge-? reading comprehension,,Sakaguchi et al. 2020,TRUE
super_glue,cb,NLI,cls,"""for multi-class F1 we compute the unweighted average of the F1 per class.""",,,BASE,250,0,mean_multiclass_f1;accuracy,,https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf,,superglue-cb,cls/nli,sentence pair,knowledge-neutral inference,,de Marneffe et al. 2019,TRUE
super_glue,rte,NLI,cls,,,,BASE,2490,0,accuracy,,https://arxiv.org/pdf/1905.00537.pdf,,superglue-rte,cls/nli,sentence pair,knowledge modest inference,,Dagan et al. 2005; Bar-Haim et al. 2006 Giampiccolo et al. 2007; Bentivogli et al. 2009,TRUE
anli,,NLI,cls,"In addition to accuracy, paper also evaluates on range of relaxed/strict and matched/unmatched settings and reports F scores for different answers",,,BASE,162865,0,accuracy,,https://arxiv.org/abs/1910.14599,,anli,cls/nli,sentence pair,knowledge modest inference,,Nie et al. 2020,TRUE
glue,mrpc,paraphrase,cls,,,BASE,,3668,3668,accuracy;f1_score,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/I05-50025B15D.pdf,,glue-mrpc,cls/paraphrase,,paraphrase,,Dolan and Brockett 2005,TRUE
glue,qqp,paraphrase,cls,,,BASE,,363846,363846,accuracy;f1_score,,https://aclanthology.org/I05-5002.pdf,,glue-qqp,cls/paraphrase,,,,(link),TRUE
paws,labeled_final,paraphrase,cls,,,BASE,,49401,49401,,,,,paws,cls/paraphrase,,,,Zhang et al. 2019,TRUE
ai2_arc,ARC-Challenge,QA_closed_book,cls,,GPT,GPT_EVAL,,1119,0,"accuracy_with_tie : For each question, a system receives 1 point if it
chooses the correct answer and 1/k if it reports a k-way tie
(i.e., chooses multiple answers) that includes the correct answer.",,https://arxiv.org/pdf/1803.05457.pdf,mid-intensive,ARC (chal.),qa/multiple-choice qa,,nontrivial_comprehension,,Clark et al. 2018,TRUE
ai2_arc,ARC-Easy,QA_closed_book,cls,,GPT,GPT_EVAL,,2251,0,"accuracy_with_tie: For each question, a system receives 1 point if it
chooses the correct answer and 1/k if it reports a k-way tie
(i.e., chooses multiple answers) that includes the correct answer.",,https://arxiv.org/pdf/1803.05457.pdf,mid-intensive,ARC (easy),Multiple choice,,,,,TRUE
kilt_tasks,hotpotqa,QA_closed_book,gen,recast as closed-book due to input length,self,BASE,,88869,88869,,,,,kilt hotpotqa,qa/closed-book qa,,encyclopedia; multi-hop QA,,Yang et al. 2018,TRUE
trivia_qa,unfiltered,QA_closed_book,gen,,GPT,GPT_EVAL,,87622,0,exact_match;f1_over_words => wikipedia aliases are considered valid answers,TRUE,https://arxiv.org/pdf/1705.03551.pdf,intensive,Trivia QA,,,,,,TRUE
web_questions,,QA_closed_book,gen,"""supposed to be answerable by Freebase"" Check corpora deduplication with freebaseqa.",GPT,GPT_EVAL,,3778,0,accuracy : they don't mention how they normalize across multiple correct answers,TRUE,https://aclanthology.org/D13-1160.pdf,intensive,web questions,qa/closed-book qa,,,,Berant et al. 2013,TRUE
wiki_qa,,QA_closed_book,cls,,CrossFit,BASE,,20360,20360,,,https://aclanthology.org/D15-1237.pdf,,wiki qa,cls/other,,,,Yang et al. 2015,TRUE
adversarial_qa,dbidaf,QA_extractive,ext,,,BASE,,10000,10000,,,https://aclanthology.org/2020.tacl-1.43/,,adversarialqa,qa/machine reading comprehension,,,,Bartolo et al. 2020,TRUE
adversarial_qa,dbert,QA_extractive,ext,,,BASE,,10000,10000,,,,,,,,,,,TRUE
adversarial_qa,droberta,QA_extractive,ext,,,BASE,,10000,10000,,,,,,,,,,,TRUE
duorc,SelfRC,QA_extractive,ext,,TaskEmbed;CrossFit,BASE,,60721,60721,,,https://duorc.github.io/,,DuoRC,qa/machine reading comprehension,,,Wikipedia/IMDB crowd,Saha et al. 2018,TRUE
duorc,ParaphraseRC,QA_extractive,ext,,TaskEmbed;CrossFit,BASE,,69524,69524,,,https://arxiv.org/pdf/1804.07927.pdf,,DuoRC,paraphrased QA,,,,Saha et al. 2018,TRUE
ropes,,QA_extractive,ext,,,BASE,,10924,10924,,,,modest,ropes,Extractive QA,,cause_and_effect;nontrivial_comprehension,,Lin et al. 2019,TRUE
squad_v2,,QA_extractive,ext,,GPT,GPT_EVAL,,130319,0,exact_match;f1_score,TRUE,https://arxiv.org/pdf/1806.03822.pdf,,SQuAD 2.0,Extractive QA,,,,Rajpurkar et al. 2018,TRUE
super_glue,record,QA_extractive,ext,,,SGLUE,,100730,0,max_token_level_f1;exact_match,TRUE,https://arxiv.org/pdf/1810.12885.pdf,,superglue-record,qa/machine reading comprehension,,knowledge-? reading comprehension,,Zhang et al. 2018,TRUE
quoref,,QA_extractive,ext,,,BASE,,19399,19399,,,https://aclanthology.org/D19-1606.pdf,,Quoref,Extractive QA,,,,Dasigi et al. 2019,TRUE
cos_e,v1.11,QA_multiple_choice,cls,"same as commonsense_qa but with (poorly sourced) human explanations; questionable ""commonsense"" lots of world knowledge",Vania,BASE,,9741,9741,,,,,cos e,other/generate explanation,,,,Rajani et al. 2019,TRUE
cosmos_qa,,QA_multiple_choice,cls,,,BASE,,25262,25262,,,,,cosmos qa,qa/multiple-choice qa,,,,Huang et al. 2019,TRUE
dream,,QA_multiple_choice,cls,,,BASE,,6116,6116,,,,,dream,qa/multiple-choice qa,,,,Sun et al. 2019,TRUE
openbookqa,main,QA_multiple_choice,cls,interesting combo of pragmatics + scientific reasoning,GPT,GPT_EVAL,,4957,0,"accuracy_with_tie : For each question, a system receives 1 point if it
chooses the correct answer and 1/k if it reports a k-way tie
(i.e., chooses multiple answers) that includes the correct answer.",,https://aclanthology.org/D18-1260.pdf,modest,openbookqa,qa/multiple-choice qa,,pragmatics,,Mihaylov et al. 2018,TRUE
qasc,,QA_multiple_choice,cls,,,BASE,,8134,8134,,,,given?,qasc,qa/multiple-choice qa,,,,Khot et al. 2020,TRUE
quail,,QA_multiple_choice,cls,,,BASE,,10246,10246,,,,,quail,qa/multiple-choice qa,,,,Rogers et al. 2020,TRUE
quarel,,QA_multiple_choice,cls,,CrossFit,BASE,,1941,1941,,,,,quarel,qa/multiple-choice qa,,logical form,,Tafjord et al. 2019a,TRUE
quartz,,QA_multiple_choice,cls,,,BASE,,2696,2696,,,https://aclanthology.org/D19-1608.pdf,given?,quartz-with knowledge,qa/multiple-choice qa,,,,Tafjord et al. 2019b,TRUE
race,high,QA_multiple_choice,cls,GPT-hard,GPT,GPT_EVAL,,62445,0,accuracy,,https://arxiv.org/pdf/1704.04683.pdff,neutral,race-high,qa/multiple-choice qa,,knowledge-neutral reading comprehension,,Lai et al. 2017,TRUE
race,middle,QA_multiple_choice,cls,"revisit: define as comprehension, paragraph level?",GPT,GPT_EVAL,,25421,0,accuracy,,https://arxiv.org/pdf/1704.04683.pdf,neutral,race-middle,qa/multiple-choice qa,,knowledge-neutral reading comprehension,,Lai et al. 2017,TRUE
sciq,,QA_multiple_choice,cls,,,BASE,,11679,11679,,,,,sciq,qa/multiple-choice qa,,,,Welbl et al. 2017,TRUE
social_i_qa,,QA_multiple_choice,cls,metric differ by prompt: 4-way classification cast as binary ,,BASE,,33410,33410,accuracy,,https://arxiv.org/pdf/1904.09728.pdf,,SIQA,qa/multiple-choice qa,,cultural knowledge,,Sap et al. 2019,TRUE
super_glue,boolq,QA_multiple_choice,cls,,,SGLUE,,9427,0,accuracy,,https://arxiv.org/pdf/1905.10044.pdf,neutral?,superglue-boolq,,,knowledge-? reading comprehension,,,TRUE
super_glue,copa,QA_multiple_choice,cls,,,SGLUE,BASE,400,0,accuracy,,http://commonsensereasoning.org/2011/papers/Roemmele.pdf,modest,superglue-copa,qa/multiple-choice qa,,causal cognition,,Gordon et al. 2012,TRUE
super_glue,multirc,QA_multiple_choice,cls,F1 over all answer options. See paper p. 259 for defintion,,SGLUE,,27243,0,f1_over_all_options;exact_match,,https://aclanthology.org/N18-1023.pdf,neutral?,superglue-multirc,qa/multiple-choice qa,,knowledge-? reading comprehension,,Khashabi et al. 2018,TRUE
wiki_hop,original,QA_multiple_choice,cls,,,BASE,,43738,43738,,,https://transacl.org/ojs/index.php/tacl/article/viewFile/1325/299,,WikiHop (Welbl et al. 2018),multi-hop QA,,,Wikipedia KB,,TRUE
wiqa,,QA_multiple_choice,cls,,,BASE,,29808,29808,,,,,wiqa,qa/multiple-choice qa,,cause_and_effect,,Tandon et al. 2019,TRUE
piqa,,QA_multiple_choice,cls,revisit: not just other,GPT,GPT_EVAL,,16113,0,accuracy,,https://arxiv.org/pdf/1911.11641.pdf,,PIQA,Multiple choice,,physical_cognition,,Bisk et al. 2020,TRUE
amazon_polarity,,sentiment,cls,,,BASE,,3600000,500000,,,https://cs.stanford.edu/people/jure/pubs/reviews-recsys13.pdf,,amazon polarity,cls/sentiment analysis,,,,McAuley and Leskovec 2013,TRUE
app_reviews,,sentiment,cls,,,BASE,,288065,288065,,,,,app reviews,other/regression,,,,Missing,TRUE
imdb,,sentiment,cls,,,BASE,,25000,25000,,,,,imdb,cls/sentiment analysis,,no dev set,,Maas et al. 2011,TRUE
rotten_tomatoes,,sentiment,cls,,,BASE,,8530,8530,,,,,rotten tomatoes,cls/sentiment analysis,,,,Pang and Lee 2005,TRUE
yelp_review_full,,sentiment,cls,no dev set,,BASE,,650000,500000,,,,,yelp review full,other/regression,,,,Zhang et al. 2015; (link),TRUE
story_cloze,2016,story_completion,cls,todo: custom loading; swag like?,GPT,,BASE,,0,accuracy,,https://arxiv.org/pdf/1604.01696.pdf,,,,,,,,TRUE
hellaswag,,story_completion,cls,,GPT,GPT_EVAL,BASE,39905,0,accuracy,,https://arxiv.org/pdf/1905.07830.pdf,,hellaswag,qa/multiple-choice qa,,,,Zellers et al. 2019,TRUE
common_gen,,structure_to_text,gen,,,BASE,,67389,67389,,,,,common gen,other,,,,Lin et al. 2020b,TRUE
wiki_bio,,structure_to_text,gen,,,BASE,,582659,500000,,,,,wiki bio,cg/other,,,,Lebret et al. 2016,TRUE
cnn_dailymail,3.0.0,summarization,gen,,,BASE,,287113,287113,,,,,,,,,,,TRUE
gigaword,,summarization,gen,,,BASE,,3803957,500000,,,,,gigaword,cg/summarization,,,,Napoles et al. 2012,TRUE
multi_news,,summarization,gen,,CrossFit,BASE,,44972,44972,,,,,multi news,cg/summarization,,,,Fabbri et al. 2019,TRUE
samsum,,summarization,gen,,CrossFit,BASE,,14732,14732,,,,,samsum,cg/summarization,,,,Gliwa et al. 2019,TRUE
xsum,,summarization,gen,,,BASE,,204045,204045,rouge,,https://arxiv.org/pdf/1808.08745.pdf,,xsum,cg/summarization,,,,Narayan et al. 2018,TRUE
ag_news,,topic_classification,cls,,,BASE,,120000,120000,,,http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html,,ag news,cls/topic,,,,Gulli (link),TRUE
dbpedia_14,,topic_classification,cls,,,BASE,,560000,500000,,,https://svn.aksw.org/papers/2013/SWJ_DBpedia/public.pdf,,dbpedia 14,cls/topic,,,,Lehmann et al. 2015,TRUE
trec,,topic_classification,cls,,,BASE,,5452,5452,,,https://trec.nist.gov/data/qa.html,,trec,cls/other,,,,Li and Roth 2002; Hovy et al. 2001,TRUE
super_glue,wic,word_sense_disambiguation,cls,,,SGLUE,BASE,5428,0,accuracy,,https://arxiv.org/pdf/1808.09121.pdf,,superglue-wic,cls/other,,lexical_knowledge,,Pilehvar and Camacho-Collados 2019,TRUE
